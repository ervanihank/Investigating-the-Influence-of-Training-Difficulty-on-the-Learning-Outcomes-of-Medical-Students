---
title: "optimal_difficulty_lmer_analysis"
author: "Erva"
date: "2024-04-26"
output: 
  html_document: default
editor_options: 
  chunk_output_type: console
RStudio_version: "2023.12.0+369 Ocean Storm Release (33206f75bd14d07d84753f965eaa24756eda97b7, 2023-12-17)"
platform: "Windows 10 x64 (build 19045)"
R_version: "4.2.2 (2022-10-31 ucrt)"
---


Packages

```{r echo=FALSE}
library(data.table) # version: 1.16.4
library(dplyr) # version 1.1.4
library(ggplot2) # verison 3.4.2
library(dplyr) # version 1.1.4
library(tools) # version 4.2.2
library(progress) # version 1.2.3
library(viridis) # version 0.6.2
library(ggforce) # 0.4.1
library(see) # version 0.7.5
library(stringr) # version 1.5.0
library(reshape2) # version 1.4.4
library(gridExtra) # version 2.3
library(lme4) # version: 1.1.31
library(lmerTest) # version 3.1.3
library(sjPlot) # version 2.8.15
library(performance) # version 0.12.4
library(tibble) # version 3.2.1
library(corrplot) # version 0.92
library(moments) # version 0.14.1
library(nortest) # version 1.0.4
library(lmtest) # version 0.9.40
library(parameters) # version 0.24.0
library(purrr) # version 1.0.1
```

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/Ghislaine/Desktop/optimal_difficulty_1/code/data/sides/elo_bins')

setwd('C:/Users/Ghislaine/Desktop/optimal_difficulty_1/code/data/sides/elo_bins')
```

# read file
```{r}
file= "average_quest_dif_100.csv"
# read the file
average_quest_dif <- read.csv(file)

  name_file <- substr(file, 18, nchar(file))

```

# rename variables to make them transparent
```{r}
# rename the column
average_quest_dif <- average_quest_dif %>%
  rename(mean_relative_difficulty = mean_skill_difficulty_difference)

# rename the column
average_quest_dif <- average_quest_dif %>%
  rename(student_ability = ability_2020_2021)

# rename the column
average_quest_dif <- average_quest_dif %>%
  rename(relative_difficulty_slope = slope)
```


# mean- center interaction variables  (only the n_question_in_spec_training)
```{r}
# Calculate the mean of n_question_in_spec_training
mean_n_question <- mean(average_quest_dif$n_question_in_spec_training, na.rm = TRUE)

# Center the variable n_question_in_spec_training by subtracting its mean from each observation
average_quest_dif$n_question_in_spec_training_c <- average_quest_dif$n_question_in_spec_training - mean_n_question

```

# correaltions
## correlation matrix
```{r}

# Selecting the columns
for_table <- average_quest_dif[c('mean_relative_difficulty', 'student_ability', 'prop_correct_ecn', 'mean_student_average_ability', 'n_question_in_spec_training', 'mean_difficulty','mean_elo_ExpectedScore','n_question_in_spec_training_c')]


# Calculating means and standard deviations
means <- colMeans(for_table, na.rm = TRUE)
sds <- apply(for_table, 2, sd, na.rm = TRUE)

# Correlation matrix
correlations <- cor(for_table, use = 'pairwise.complete.obs')

# Create a formatted table with means, SDs, and correlations
formatted_table <- data.frame(M = means, SD = sds, correlations)

# Function to determine significance asterisks
significance_asterisks <- function(p_value) {
  if (p_value < 0.001) {
    return('***')
  } else if (p_value < 0.01) {
    return('**')
  } else if (p_value < 0.05) {
    return('*')
  } else {
    return('')
  }
}

# Populate the correlations with significance testing
for (col in colnames(for_table)) {
  for (row in colnames(for_table)) {
    if (col == row) {
      # Diagonals are not displayed in the table
      formatted_table[row, col] <- 'â€”'
    } else {
      # Calculate the p-value
      p_value <- Hmisc::rcorr(as.matrix(for_table[, c(row, col)]))$P[1, 2]
      # Format with two decimal places and add significance asterisks
      corr <- correlations[row, col]
      formatted_table[row, col] <- paste0(format(corr, digits = 2), significance_asterisks(p_value))
    }
  }
}

# You can save this table to a CSV or Excel file, or print it out
write.csv(formatted_table, file = 'formatted_correlation_table_with_significance.csv', row.names = TRUE)

formatted_table


# Calculating the correlation matrix
correlation_matrix <- cor(for_table, use = "pairwise.complete.obs")

# Print the correlation matrix
print(correlation_matrix)


# visiulazi
# Compute correlation matrix
correlation_matrix <- cor(for_table)

# Visualize the correlation matrix
library(corrplot)
corrplot(correlation_matrix, method = "number")


```



## Full Model & Colinearity check : specialty as radnom intercept and slope
```{r}
model_formula <- prop_correct_ecn ~ I(mean_relative_difficulty^2) + mean_relative_difficulty + 
  n_question_in_spec_training_c+
  student_ability+
  student_ability:I(mean_relative_difficulty^2)+
  student_ability:mean_relative_difficulty+
  student_ability:n_question_in_spec_training_c+
  (1 | student)+
  (1 + mean_relative_difficulty + I(mean_relative_difficulty^2) | specialty)

# Fit the linear mixed-effects model
model_fit <- lmer(model_formula, data = average_quest_dif, control = lmerControl(optimizer = "bobyqa"))

# Display the model summary with the name of the file
cat(paste("Model summary for", name_file, ":\n"))
print(summary(model_fit))
# Create a table of fixed effects
tabel_model <- tab_model(model_fit, show.se = TRUE, title = "Table: Regression Analysis Results")

# Print the table
tabel_model

## CHECK COLINEARITY
library(performance)
check_collinearity(model_fit)


## AIC and BIC - Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC)
aic_value <- AIC(model_fit)
bic_value <- BIC(model_fit)
cat("AIC =", aic_value, "\n")
cat("BIC =", bic_value, "\n")

```
### Assumption checks

#### for DV
```{r}
# Histogram of the DV
hist(average_quest_dif$prop_correct_ecn, 
     main = "Histogram of the Dependent Variable", 
     xlab = "Final Exam Score (Proportion Correct)", 
     breaks = 30, col = "gray")

# Q-Q plot of the DV
qqnorm(average_quest_dif$prop_correct_ecn, main = "Q-Q Plot of the Dependent Variable")
qqline(average_quest_dif$prop_correct_ecn, col = "red")

# Skewness and Kurtosis
skewness_value <- skewness(average_quest_dif$prop_correct_ecn)
kurtosis_value <- kurtosis(average_quest_dif$prop_correct_ecn)
print(paste("Skewness:", skewness_value))
print(paste("Kurtosis:", kurtosis_value))


# Box plot for the dependent variable
boxplot(average_quest_dif$prop_correct_ecn,
        main = "Box Plot of Dependent Variable",
        ylab = "Final Exam Score (Proportion Correct)",
        col = "lightblue")


```

#### for residuals
```{r}
# linearity
# Plot observed vs fitted values
fitted_values <- fitted(model_fit)
plot(fitted_values, average_quest_dif$prop_correct_ecn,
     main = "Observed vs Fitted Values",
     xlab = "Fitted Values", ylab = "Observed Values")
abline(a = 0, b = 1, col = "red")  # Add a diagonal line

# Residuals vs Fitted Values
residuals <- resid(model_fit)
plot(fitted_values, residuals,
     main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a horizontal line at 0

residuals <- resid(model_fit)

# Q-Q plot
qqnorm(residuals)
qqline(residuals, col = "red")

# Histogram
hist(residuals, breaks = 50, main = "Histogram of Residuals", xlab = "Residuals")

skewness_value <- skewness(residuals)
kurtosis_value <- kurtosis(residuals)

print(paste("Skewness:", skewness_value))
print(paste("Kurtosis:", kurtosis_value))

plot(density(residuals), main = "Density Plot of Residuals", xlab = "Residuals")
curve(dnorm(x, mean = mean(residuals), sd = sd(residuals)), add = TRUE, col = "red")
legend("topright", legend = c("Empirical", "Normal"), col = c("black", "red"), lty = 1)

# Box plot for residuals
residuals <- resid(model_fit)  # Calculate residuals from the fitted model
boxplot(residuals,
        main = "Box Plot of Residuals",
        ylab = "Residuals",
        col = "lightgreen")
```


### p value for random effect
```{r}
library(lme4)
full_model <- lmer(prop_correct_ecn ~ I(mean_relative_difficulty^2) + mean_relative_difficulty + 
  n_question_in_spec_training_c +
  student_ability +
  student_ability:I(mean_relative_difficulty^2) +
  student_ability:mean_relative_difficulty +
  student_ability:n_question_in_spec_training_c +
  (1 | student) +
  (1 + mean_relative_difficulty + I(mean_relative_difficulty^2) | specialty), 
  data = average_quest_dif, control = lmerControl(optimizer = "bobyqa"), REML = FALSE)


reduced_model <- lmer(prop_correct_ecn ~ I(mean_relative_difficulty^2) + mean_relative_difficulty + 
  n_question_in_spec_training_c +
  student_ability +
  student_ability:I(mean_relative_difficulty^2) +
  student_ability:mean_relative_difficulty +
  student_ability:n_question_in_spec_training_c +
  (1 | student) +
  (1 + I(mean_relative_difficulty^2) | specialty), 
  data = average_quest_dif, control = lmerControl(optimizer = "bobyqa"), REML = FALSE)

library(lmtest)
anova_result <- anova(reduced_model, full_model)
print(anova_result)

```



##have table with exact numbers

```{r}
# Assume we already have the summary data frame
fixed_effects <- summary(model_fit)$coefficients

# Check if p-values are below .Machine$double.eps and adjust display
fixed_effects[,"Pr(>|t|)"] <- ifelse(fixed_effects[,"Pr(>|t|)"] < .Machine$double.eps,
                                     "< 1e-16",  # You can choose the threshold
                                     format.pval(fixed_effects[,"Pr(>|t|)"], digits = 10))

# Print the modified fixed effects table
print(fixed_effects)

```

# Model Fit
```{r}
## PLOT
  
  # Create a sequence of mean_relative_difficulty values for plotting
  x_values <- seq(
    min(average_quest_dif$mean_relative_difficulty ),
    max(average_quest_dif$mean_relative_difficulty ),
    length.out = 20
  )

  x_values_slope <- seq(
    min(average_quest_dif$relative_difficulty_slope ),
    max(average_quest_dif$relative_difficulty_slope ),
    length.out = 20
  )
  
   x_values_ability <- seq(
    min(average_quest_dif$student_ability ),
    max(average_quest_dif$student_ability ),
    length.out = 20
  )
   
   x_values_nb_question<-seq(
    min(average_quest_dif$n_question_in_spec_training_c ),
    max(average_quest_dif$n_question_in_spec_training_c ),
    length.out = 10
  )
   
  # List of unique specialties
  specialties <- unique(average_quest_dif$specialty)
  
  
  # Create a data frame to store the values for prediction
prediction_data <- expand.grid(
  mean_relative_difficulty = x_values,
  specialty = unique(average_quest_dif$specialty),
  student_ability = x_values_ability,
  n_question_in_spec_training_c=mean(average_quest_dif$n_question_in_spec_training_c ),
  relative_difficulty_slope=mean(average_quest_dif$relative_difficulty_slope )
)
  
  # Predict values using the model, including random effects for 'specialty'
  predicted_values <- predict(model_fit, newdata = prediction_data, re.form = ~   (1 + mean_relative_difficulty + I(mean_relative_difficulty^2) | specialty))
  
  prediction_data$learning_slope = predicted_values
  
  # Convert student to factor
  average_quest_dif <- average_quest_dif %>%
    mutate(student = factor(student))
  
  library(ggplot2)
  # 
  # # find the mean of mean relative diffciulty
  # mean_mean_relative_diffciulty= prediction_data %>%
  # group_by(specialty) %>%
  #    summarise(mean_mean = mean(mean_relative_difficulty))

# Find the mean_relative_difficulty values that maximize the predicted prop_correct_ecn for each specialty and student_ability
max_x_values <- prediction_data %>%
  group_by(specialty, student_ability) %>%
  summarise(max_mean_difficulty = mean_relative_difficulty[which.max(learning_slope)])

  # Calculate the number of points in each facet for each specialty
facet_counts <- average_quest_dif %>%
  group_by(specialty) %>%
  summarise(n = n())
  
p <- ggplot(prediction_data, aes(x = mean_relative_difficulty, y = learning_slope)) +
  geom_line(aes(group = interaction(specialty, student_ability), color = student_ability),
            size = 2, alpha = 0.7) +
  labs(
    x = expression(atop("Mean Relative Difficulty", atop("Mean(Question Difficulty - Online Student Ability)", ""))),
    y = expression(atop("Final Exam Score", atop("(Proportion Correct)", "")))
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text = element_text(size = 18),
    axis.title = element_text(size = 20),
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 16, margin = margin(b = 10)),
    plot.caption = element_text(size = 14, margin = margin(t = 10)),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 16),
    strip.text = element_text(size = 11),
    panel.grid.major = element_line(color = "lightgray", linetype = "dashed")
  ) +
  facet_wrap(~gsub("_", " ", specialty), scales = "free", ncol = 4) +  # 4 columns
  scale_color_viridis_c(option = "C", direction = -1) +
  geom_vline(
    data = max_x_values,
    aes(xintercept = max_mean_difficulty, color = student_ability),
    size = 0.7,
    linetype = "dashed"
  ) +
  geom_text(
    data = facet_counts,
    aes(label = paste("n =", n), x = Inf, y = Inf),
    hjust = 1,
    vjust = 1,
    size = 5,
    show.legend = FALSE
  ) +
  labs(color = "Final Student Ability", linetype = "Optimal")

# Increase the width for 4 subplots per row
ggsave(
   "C:/Users/Ghislaine/Desktop/optimal_difficulty_1/code/data/sides/elo_bins/modelfit.png",
  plot = p,
  width = 19, height = 10, units = "in", dpi = 300
)

print(p)
```


## model fit for only students with mean student ability

```{r}
## PLOT
  
  # Create a sequence of mean_relative_difficulty values for plotting
  x_values <- seq(
    min(average_quest_dif$mean_relative_difficulty ),
    max(average_quest_dif$mean_relative_difficulty ),
    length.out = 20
  )

  x_values_slope <- seq(
    min(average_quest_dif$relative_difficulty_slope ),
    max(average_quest_dif$relative_difficulty_slope ),
    length.out = 20
  )
  
   x_values_ability <- seq(
    min(average_quest_dif$student_ability ),
    max(average_quest_dif$student_ability ),
    length.out = 20
  )
   
   x_values_nb_question<-seq(
    min(average_quest_dif$n_question_in_spec_training_c ),
    max(average_quest_dif$n_question_in_spec_training_c ),
    length.out = 10
  )
   
  # List of unique specialties
  specialties <- unique(average_quest_dif$specialty)
  
  
  # Create a data frame to store the values for prediction
prediction_data <- expand.grid(
  mean_relative_difficulty = x_values,
  specialty = unique(average_quest_dif$specialty),
  student_ability = mean(average_quest_dif$student_ability ),
  n_question_in_spec_training_c=mean(average_quest_dif$n_question_in_spec_training_c ),
  relative_difficulty_slope=mean(average_quest_dif$relative_difficulty_slope )
)
  
  # Predict values using the model, including random effects for 'specialty'
  predicted_values <- predict(model_fit, newdata = prediction_data, re.form = ~   (1 + mean_relative_difficulty + I(mean_relative_difficulty^2) | specialty))
  
  prediction_data$learning_slope = predicted_values
  
  # Convert student to factor
  average_quest_dif <- average_quest_dif %>%
    mutate(student = factor(student))
  
  library(ggplot2)
  # 
  # # find the mean of mean relative diffciulty
  # mean_mean_relative_diffciulty= prediction_data %>%
  # group_by(specialty) %>%
  #    summarise(mean_mean = mean(mean_relative_difficulty))

# Find the mean_relative_difficulty values that maximize the predicted prop_correct_ecn for each specialty and student_ability
max_x_values <- prediction_data %>%
  group_by(specialty, student_ability) %>%
  summarise(max_mean_difficulty = mean_relative_difficulty[which.max(learning_slope)])

  # Calculate the number of points in each facet for each specialty
facet_counts <- average_quest_dif %>%
  group_by(specialty) %>%
  summarise(n = n())
  
p <- ggplot(prediction_data, aes(x = mean_relative_difficulty, y = learning_slope)) +
  geom_line(aes(group = interaction(specialty, student_ability), color = student_ability),
            size = 2, alpha = 0.7) +
  labs(
    x = expression(atop("Mean Relative Difficulty", atop("Mean(Question Difficulty - Online Student Ability)", ""))),
    y = expression(atop("Mock Final Exam Score", atop("(Proportion Correct)", "")))
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text = element_text(size = 18),
    axis.title = element_text(size = 20),
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 16, margin = margin(b = 10)),
    plot.caption = element_text(size = 14, margin = margin(t = 10)),
    legend.title = element_text(size = 16),
    legend.text = element_text(size = 16),
    strip.text = element_text(size = 11),
    panel.grid.major = element_line(color = "lightgray", linetype = "dashed")
  ) +
  facet_wrap(~gsub("_", " ", specialty), scales = "free", ncol = 4) +  # 4 columns
  scale_color_viridis_c(option = "C", direction = -1) +
  # geom_vline(
  #   data = max_x_values,
  #   aes(xintercept = max_mean_difficulty, color = student_ability),
  #   size = 1,
  #   linetype = "dashed"
  # ) +
  labs(color = "Final Student Ability", linetype = "Optimal")
  #+geom_text(data = max_x_values, aes(x = max_mean_difficulty-1.5, y = 0.3, label = round(max_mean_difficulty, 2)), vjust = -1, size = 7.5, color = "darkgreen", hjust = -0.1)


# Increase the width for 4 subplots per row
ggsave(
   "C:/Users/Ghislaine/Desktop/optimal_difficulty_1/code/data/sides/elo_bins/modelfit.png",
  plot = p,
  width = 19, height = 10, units = "in", dpi = 300
)

print(p)
```


```{r}
library(lme4)
library(dplyr)

# Assuming model_fit is already fitted

# Calculate mean values for student_ability and n_question_in_spec_training_c
mean_student_ability <- mean(average_quest_dif$student_ability, na.rm = TRUE)
mean_n_questions <- mean(average_quest_dif$n_question_in_spec_training_c, na.rm = TRUE)

# Define a sequence of mean_relative_difficulty values to evaluate
mrd_seq <- seq(from = min(average_quest_dif$mean_relative_difficulty, na.rm = TRUE), 
               to = max(average_quest_dif$mean_relative_difficulty, na.rm = TRUE), length.out = 100)

# Function to calculate predicted prop_correct_ecn for a given specialty
predict_specialty <- function(specialty) {
  # Create a data frame for predictions
  pred_data <- expand.grid(
    mean_relative_difficulty = mrd_seq,
    student_ability = mean_student_ability,
    n_question_in_spec_training_c = mean_n_questions,
    specialty = specialty
  )
  
  # Predict prop_correct_ecn using the model
  pred_data$prop_correct_ecn <- predict(model_fit, newdata = pred_data,  re.form = ~ (1 + mean_relative_difficulty + I(mean_relative_difficulty^2) | specialty))
  
  # Find the mean_relative_difficulty that maximizes prop_correct_ecn
  max_difficulty <- pred_data$mean_relative_difficulty[which.max(pred_data$prop_correct_ecn)]
  return(max_difficulty)
}

# Apply function to each specialty
specialties <- unique(average_quest_dif$specialty)
optimal_difficulties <- sapply(specialties, predict_specialty)

# View results
names(optimal_difficulties) <- specialties
optimal_difficulties

# Calculate mean and standard deviation of optimal mean_relative_difficulties
mean_difficulty <- mean(optimal_difficulties)
sd_difficulty <- sd(optimal_difficulties)

# Print results
cat("Mean of optimal mean_relative_difficulty: ", mean_difficulty, "\n")
cat("Standard deviation of optimal mean_relative_difficulty: ", sd_difficulty, "\n")



```

